{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('TF-IDF').getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_file(x):\n",
    "    value=x.split('\\t')\n",
    "    return (value[0], value[1].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read categories into dataframe\n",
    "categories = spark.read.csv('article_categories.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Make a column for each unique category, split each category on spaces\n",
    "unique_cats = categories.select('categories').distinct()\n",
    "\n",
    "# Split unique categories on spaces\n",
    "cats_split = unique_cats.select(psf.split(psf.col('categories'), ' ').alias('categories'))\n",
    "\n",
    "# Make a list of all the categories\n",
    "cats_list = cats_split.select(psf.explode(psf.col('categories')).alias('category')).distinct()\n",
    "\n",
    "# Add to categories dataframe a column for each unique category\n",
    "for row in cats_list.collect():\n",
    "    category = row.category\n",
    "    categories = categories.withColumn(category, psf.when(categories['categories'].contains(category), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               words|\n",
      "+---+--------------------+\n",
      "|  3|[\"this, article, ...|\n",
      "|  2|[\"lorem, ipsum, d...|\n",
      "|  1|[\"lorem, ipsum, d...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = (sc.textFile('cleaned-test.txt')\n",
    "    .map(lambda x: split_file(x))\n",
    "    .toDF(['id', 'words'])\n",
    ")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/18 15:30:48 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 439:=====================>                                   (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+-------+--------+-----+-----+-----------+-----+-----+---------------+--------+-------+-------+--------------+-------+-------+---------------+--------+-----+----------------+-----+-------+-------+-----+-------+---------------+-----------+------------------+-----+-------+-------+-------+--------+--------------+-----+----------------+--------+-----+--------+-----+-----------+-----+-------+-----+-------+--------------+-------+-------+--------------+---------------+-----+-------------+--------+---------------+--------+-------+-----+--------+-----+-----+-------+--------+--------+-------+--------+--------+-------+-----+--------+--------+-----+-------+--------+-----+-------+-------+---------------+--------------+----------------+-------+-------+-----+-------+-------+-----+-------+-------+-----------------+-----+-------+-------+---------------+-------+-----+-----+--------+-------+--------+-----+--------------+-----+-----------+-------+-----+--------+-----+------+-----+-----+-------+--------+-------+---------------+--------+-------+--------+-------+-------+--------+--------+-----+-------+------------------+-----------------+-------+-----------+-------+-----+--------------+-------+-----+-------+--------+-------+--------+-------+----------------+-------+---------------+-----+-------+-------+------+------+-----+-----------------+-------+--------------+-----+-----+-----------+--------+--------------+-------+-------------+-----+-------+--------+--------------+--------+-------+-------+-------+--------+------+-------------+--------+-------+-----+--------+-----+-------+-----+------+-----+-------+-----+--------+\n",
      "| id|words|categories|nlin.CD|q-fin.CP|cs.LG|cs.MS|astro-ph.IM|cs.NE|cs.CV|physics.ins-det|q-fin.MF|math.CA|math.SP|physics.optics|math.SG|math.NA|cond-mat.str-el|q-fin.PM|cs.CE|physics.atm-clus|cs.CR|stat.AP|math.CT|gr-qc|math.KT|physics.hist-ph|astro-ph.SR|cond-mat.stat-mech|cs.OH|math.HO|math.OA|eess.SY|q-fin.EC|physics.pop-ph|cs.MM|physics.class-ph|q-bio.TO|cs.PF|funct-an|cs.CG|astro-ph.GA|cs.SC|econ.TH|cs.HC|math.QA|physics.gen-ph|math.MP|math.MG|physics.acc-ph|physics.comp-ph|cs.NA|physics.ao-ph|q-fin.RM|physics.chem-ph|q-bio.SC|math.DG|cs.DS|q-bio.OT|cs.GL|cs.DC|math.AG|cond-mat|q-bio.PE|math.FA|supr-con|chao-dyn|stat.ME|cs.AR|q-fin.PR|q-fin.TR|q-alg|math.AT|q-bio.MN|dg-ga|nlin.CG|math.PR|physics.atom-ph|cond-mat.other|physics.space-ph|math.GR|math.DS|cs.DM|math.RA|nlin.SI|cs.SD|nlin.PS|eess.IV|cond-mat.supr-con|cs.NI|math.LO|chem-ph|physics.data-an|stat.CO|cs.CC|q-bio|quant-ph|eess.AS|q-bio.QM|cs.GR|physics.soc-ph|cs.IT|astro-ph.HE|nucl-ex|cs.OS|q-fin.GN|cs.AI|hep-th|cs.CY|cs.LO|math.GN|q-bio.GN|math.GM|cond-mat.dis-nn|q-bio.BM|math.AP|astro-ph|math.AC|nlin.AO|alg-geom|bayes-an|cs.FL|math.RT|cond-mat.quant-gas|cond-mat.mtrl-sci|econ.GN|astro-ph.EP|nucl-th|cs.PL|physics.med-ph|stat.OT|cs.SE|mtrl-th|q-bio.CB|math.NT|q-fin.ST|stat.ML|physics.plasm-ph|eess.SP|physics.flu-dyn|cs.DL|atom-ph|math.CV|hep-ph|hep-ex|cs.IR|cond-mat.mes-hall|stat.TH|physics.geo-ph|cs.RO|cs.SY|astro-ph.CO|plasm-ph|physics.bio-ph|hep-lat|physics.ed-ph|cs.DB|math.GT|q-bio.NC|physics.app-ph|adap-org|econ.EM|math.ST|math.OC|solv-int|cmp-lg|cond-mat.soft|patt-sol|math-ph|cs.CL|comp-gas|cs.GT|math.IT|cs.MA|ao-sci|cs.ET|math.CO|cs.SI|acc-phys|\n",
      "+---+-----+----------+-------+--------+-----+-----+-----------+-----+-----+---------------+--------+-------+-------+--------------+-------+-------+---------------+--------+-----+----------------+-----+-------+-------+-----+-------+---------------+-----------+------------------+-----+-------+-------+-------+--------+--------------+-----+----------------+--------+-----+--------+-----+-----------+-----+-------+-----+-------+--------------+-------+-------+--------------+---------------+-----+-------------+--------+---------------+--------+-------+-----+--------+-----+-----+-------+--------+--------+-------+--------+--------+-------+-----+--------+--------+-----+-------+--------+-----+-------+-------+---------------+--------------+----------------+-------+-------+-----+-------+-------+-----+-------+-------+-----------------+-----+-------+-------+---------------+-------+-----+-----+--------+-------+--------+-----+--------------+-----+-----------+-------+-----+--------+-----+------+-----+-----+-------+--------+-------+---------------+--------+-------+--------+-------+-------+--------+--------+-----+-------+------------------+-----------------+-------+-----------+-------+-----+--------------+-------+-----+-------+--------+-------+--------+-------+----------------+-------+---------------+-----+-------+-------+------+------+-----+-----------------+-------+--------------+-----+-----+-----------+--------+--------------+-------+-------------+-----+-------+--------+--------------+--------+-------+-------+-------+--------+------+-------------+--------+-------+-----+--------+-----+-------+-----+------+-----+-------+-----+--------+\n",
      "+---+-----+----------+-------+--------+-----+-----+-----------+-----+-----+---------------+--------+-------+-------+--------------+-------+-------+---------------+--------+-----+----------------+-----+-------+-------+-----+-------+---------------+-----------+------------------+-----+-------+-------+-------+--------+--------------+-----+----------------+--------+-----+--------+-----+-----------+-----+-------+-----+-------+--------------+-------+-------+--------------+---------------+-----+-------------+--------+---------------+--------+-------+-----+--------+-----+-----+-------+--------+--------+-------+--------+--------+-------+-----+--------+--------+-----+-------+--------+-----+-------+-------+---------------+--------------+----------------+-------+-------+-----+-------+-------+-----+-------+-------+-----------------+-----+-------+-------+---------------+-------+-----+-----+--------+-------+--------+-----+--------------+-----+-----------+-------+-----+--------+-----+------+-----+-----+-------+--------+-------+---------------+--------+-------+--------+-------+-------+--------+--------+-----+-------+------------------+-----------------+-------+-----------+-------+-----+--------------+-------+-----+-------+--------+-------+--------+-------+----------------+-------+---------------+-----+-------+-------+------+------+-----+-----------------+-------+--------------+-----+-----+-----------+--------+--------------+-------+-------------+-----+-------+--------+--------------+--------+-------+-------+-------+--------+------+-------------+--------+-------+-----+--------+-----+-------+-----+------+-----+-------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join categories with data\n",
    "joined_data = data.join(categories, [\"id\"])\n",
    "joined_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashingTF = HashingTF(inputCol=\"words\", outputCol='features', numFeatures=100000)\n",
    "tf = hashingTF.transform(joined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='features', outputCol='idf')\n",
    "model = idf.fit(tf)\n",
    "tf_idf = model.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/18 15:19:52 WARN DAGScheduler: Broadcasting large task binary with size 1639.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "|  i|  j|cosine_similarity|\n",
      "+---+---+-----------------+\n",
      "|  1|  2|             0.34|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/18 15:19:53 WARN DAGScheduler: Broadcasting large task binary with size 1639.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "|  i|  j|cosine_similarity|\n",
      "+---+---+-----------------+\n",
      "|  1|  3|              0.0|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/18 15:19:53 WARN DAGScheduler: Broadcasting large task binary with size 1639.9 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----------------+\n",
      "|  i|  j|cosine_similarity|\n",
      "+---+---+-----------------+\n",
      "|  1|  3|              0.0|\n",
      "+---+---+-----------------+\n",
      "\n",
      "+---+---+-----------------+\n",
      "|  i|  j|cosine_similarity|\n",
      "+---+---+-----------------+\n",
      "|  2|  3|              0.0|\n",
      "+---+---+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/18 15:19:54 WARN DAGScheduler: Broadcasting large task binary with size 1639.9 KiB\n"
     ]
    }
   ],
   "source": [
    "for row in cats_list.collect():\n",
    "    category = row.category\n",
    "    test = tf_idf.filter(tf_idf[f\"`{category}`\"] == 0)\n",
    "\n",
    "    cosine_similarity_udf = psf.udf(lambda x,y: round(float(x.dot(y)/(x.norm(2)*y.norm(2))), 2), DoubleType())\n",
    "\n",
    "    res = (test.alias(\"i\").join(test.alias(\"j\"), psf.col(\"i.id\") < psf.col(\"j.id\"))\n",
    "        .select(\n",
    "            psf.col(\"i.id\").alias(\"i\"), \n",
    "            psf.col(\"j.id\").alias(\"j\"), \n",
    "            cosine_similarity_udf(\"i.idf\", \"j.idf\").alias(\"cosine_similarity\"))\n",
    "        .sort(\"i\", \"j\"))\n",
    "    res.show(5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
