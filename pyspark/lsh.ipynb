{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import NGram, BucketedRandomProjectionLSH\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.sql.types import StructType,StructField, StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/17 20:02:07 WARN Utils: Your hostname, adneovrebo.local resolves to a loopback address: 127.0.0.1; using 192.168.68.108 instead (on interface en0)\n",
      "22/03/17 20:02:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/17 20:02:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/17 20:02:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/03/17 20:02:09 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Create spark session with increased memory\n",
    "spark = (SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"lsh\").config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split on tab and create a new column with rdd and split article_text into array of words\n",
    "rdd = (sc.textFile('cleaned.txt')\n",
    "        .map(lambda line: line.split('\\t'))\n",
    "        .map(lambda r: (r[0], r[1].split(\" \"))))\n",
    "\n",
    "schema = StructType([\n",
    "        StructField('id', StringType()),\n",
    "        StructField('words', ArrayType(elementType=StringType()))\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(rdd, schema)\n",
    "\n",
    "\n",
    "# # Make ngrams of size n\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngram_df = ngram.transform(df)\n",
    "\n",
    "# # # # Countvectorizer\n",
    "cv = CountVectorizer(inputCol=\"ngrams\", outputCol=\"features\", vocabSize=100_000, minDF=2)\n",
    "cv_model = cv.fit(ngram_df)\n",
    "cv_df = cv_model.transform(ngram_df)\n",
    "\n",
    "brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\", bucketLength=1_000_000,\n",
    "                                  numHashTables=100)\n",
    "model = brp.fit(cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/17 20:13:37 WARN DAGScheduler: Broadcasting large task binary with size 1411.6 KiB\n",
      "22/03/17 20:13:37 WARN DAGScheduler: Broadcasting large task binary with size 1411.6 KiB\n",
      "22/03/17 20:13:37 WARN DAGScheduler: Broadcasting large task binary with size 1411.6 KiB\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "22/03/17 20:13:38 WARN DAGScheduler: Broadcasting large task binary with size 77.7 MiB\n",
      "[Stage 36:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|               id|               words|              ngrams|            features|              hashes|           distCol|\n",
      "+-----------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "|  \"hep-ph9602258\"|[\"citex12, citeac...|[\"citex12 citeaci...|(100000,[0,1,2,3,...|[[-1.0], [-1.0], ...|29.866369046136157|\n",
      "|      \"0809.4539\"|[\"the, discovery,...|[\"the discovery, ...|(100000,[0,1,2,3,...|[[-1.0], [0.0], [...| 57.60208329565867|\n",
      "|  \"hep-ph0207019\"|[\"when, confronti...|[\"when confrontin...|(100000,[0,1,2,3,...|[[-1.0], [-1.0], ...| 57.86190456595773|\n",
      "| \"hep-lat9911007\"|[\"a, rather, comp...|[\"a rather, rathe...|(100000,[0,1,2,5,...|[[-1.0], [0.0], [...| 58.98304841223451|\n",
      "|     \"1505.07378\"|[\"neutrons, emitt...|[\"neutrons emitte...|(100000,[0,1,3,5,...|[[-1.0], [0.0], [...|              59.0|\n",
      "|\"astro-ph9805189\"|[\"almost, all, hg...|[\"almost all, all...|(100000,[0,1,2,3,...|[[0.0], [0.0], [-...| 59.08468498688979|\n",
      "|      \"0810.3393\"|[\"the, golf, ng, ...|[\"the golf, golf ...|(100000,[0,1,2,3,...|[[-1.0], [-1.0], ...| 59.12698199637793|\n",
      "|      \"1309.3865\"|[\"since, the, mid...|[\"since the, the ...|(100000,[0,1,2,3,...|[[-1.0], [-1.0], ...|59.413803110051795|\n",
      "|  \"hep-ph0507154\"|[\"one, of, the, m...|[\"one of, of the,...|(100000,[0,1,2,3,...|[[-1.0], [0.0], [...| 59.60704656330491|\n",
      "|      \"0910.1160\"|[\"flavor, changin...|[\"flavor changing...|(100000,[0,1,2,3,...|[[-1.0], [0.0], [...|59.632206063502295|\n",
      "+-----------------+--------------------+--------------------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/17 20:14:01 WARN DAGScheduler: Broadcasting large task binary with size 77.7 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load text from review.txt file\n",
    "text = open('review.txt', 'r').read().split(\" \")\n",
    "# add text to dataframe \n",
    "text_df = spark.createDataFrame([(text, )], ['words'])\n",
    "# Find ngrams of text\n",
    "text_ngram = ngram.transform(text_df)\n",
    "# Countvectorize text\n",
    "text_cv = cv_model.transform(text_ngram)\n",
    "# Get the key\n",
    "key = text_cv.first()[\"features\"]\n",
    "\n",
    "# Find the nearest neighbors\n",
    "res = model.approxNearestNeighbors(cv_df, key, 10)\n",
    "res.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
